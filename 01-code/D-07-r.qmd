---
title: "Practice Code: Day 7"
author:
  name: "Constantin Manuel Bosancianu"
  orcid: 0000-0001-7950-9798
  email: bosancianu@icloud.com
date: "August 6, 2019"
bibliography: ../references.bib
execute:
  eval: true
  echo: true
  warning: false
  error: false
format:
  html:
    toc: true
    code-fold: true
    toc-location: left
    theme: minty
    number-sections: true
    reference-location: margin
    embed-resources: true
---

# Introduction

We rely on two separate data sets today, each with its own characteristics. The first data comes from a sleep study.

```{r load-packages}
library(pacman)
p_load(tidyverse, ggeffects, texreg, arm, broom.mixed,
       ggthemes, broom, nlme, HLMdiag, knitr,
       kableExtra, magrittr)
```

We also define here the standard centering function we've been using so far.

```{r helpful-functions}
fun_two_SD <- function(x) {
        (x - mean(x, na.rm = TRUE)) / (2 * sd(x, na.rm = TRUE))
}
```

# Sleep study

The data was first introduced by @belenky_patterns_2003. A number of 66 volunteers were subjected to sleep deprivation in varying doses over 10 days. In our analyses here we concentrate on the 18 subjects who slept only 3 hours per night for 7 days. All subjects were allowed a period of recovery of 3 days of normal sleep (8 hours a night) following the 7 days.

Throughout the 10 days (7 x 3 hours per night, 3 x 8 hours per night) their reaction times were tested on a psychomotor vigilance test (a visual stimulus would be displayed and the subject's reaction time to it was measured).

The data is already made available in the `lme4` package, so all we need to do it load it from there.

```{r read-data-1}
data("sleepstudy")

sleepstudy %>%
    glimpse()
```

We'll only use this data for basic graphical examinations, and to test very simple *unconditional means* and *unconditional growth* models. Little more is possible, as there are no predictors to be found in the data.

The great benefit of working with smaller data sets is that we can rely more on graphical tools - these are vital in the context of longitudinal modeling in helping us understand the trajectories of change.

```{r examine-data, fig.height=9, fig.width = 9, dpi=144}
ggplot(data = sleepstudy,
       aes(x = Days,
           y = Reaction)) +
    theme_clean() +
    geom_point(size = 2.5) +
    geom_smooth(method = "lm",
                linewidth = 1.5,
                color = "red",
                se = FALSE) +
    geom_smooth(method = "loess",
                linewidth = 1.5,
                color = "cyan",
                se = FALSE) +
    facet_wrap(. ~ Subject, ncol = 6) +
    labs(x = "Days",
         y = "Reaction time in PVT") +
    scale_x_continuous(breaks = c(0, 1, 2, 3, 4,
                                  5, 6, 7, 8, 9),
                       labels = c("0", "1", "2",
                                  "3", "4", "5",
                                  "6", "7", "8", "9"))
```

**Quick task**: Can you use a snipped of code I was using when plotting varying slopes, and display on each panel the average response time in the entire cohort of 18 subjects? (a horizontal line)

Visually, we can tell there's a lot of variation across the 10 days of the experiment. Can we also get a measure of how much within-individual variation exists, as a share of total variation?

## Unconditional means

For this, we need the *unconditional means* specification. I continue the estimation as we did so far, in the `lme4` package, though this will only get us so far today.

```{r unconditional-means-1}
mlm.0 <- lmer(Reaction ~ 1 + (1 | Subject),
              data = sleepstudy)
summary(mlm.0)
```

The ICC from the model can easily be computed as $1278/(1278 + 1959)$. How do you interpret the ICC in the context of a longitudinal growth model?

## Unconditional growth

Let's move to the *unconditional growth* model. This is going to formalize what the first plot told us: (1) how do the initial score and rate of change in the response time change for individuals, but also (2) what the average baseline score and rate of change is in the sample.

```{r unconditional-growth-1}
mlm.1 <- lmer(Reaction ~ 1 + Days + (1 + Days | Subject),
              data = sleepstudy)
summary(mlm.1)
```

What is the average baseline response time in the sample on the first day of the experiment? What's the average rate of change in this response time for the sample?

**Quick task**: Can you plot how the estimated rate of growth varies for each individual in the sample? Use the code I used in the past few days to plot varying slopes between countries.

## Residuals inspection

How do the residuals look in this model?

```{r inspect-residuals-1, fig.height=6, fig.width=9, dpi=144}
df_resid <- hlm_resid(mlm.1,
                      level = 1)

ggplot(data = df_resid,
       aes(x = Days,
           y = .resid)) +
    theme_clean() +
    geom_point(size = 2.5) +
    geom_hline(yintercept = 0,
               color = "red",
               linewidth = 1.25,
               alpha = 0.35) +
    facet_wrap(. ~ Subject, ncol = 6) +
    labs(x = "Days",
         y = "Residual from unconditional growth model")
rm(df_resid)
```

The errors seem to exhibit a AR pattern to them. Sadly, we can't do much about it in `lme4`, as the ability to specify a custom variance-covariance structure for the L1 errors is not really available. The current principal developer on the package, Ben Bolker, walks through a few of the workarounds here ([https://bbolker.github.io/mixedmodels-misc/notes/corr_braindump.html](https://bbolker.github.io/mixedmodels-misc/notes/corr_braindump.html))^[Last accessed on February 1, 2023.], but I can't say either option from there is truly satisfactory for us.

This is why we need to turn to the `nlme` package. Though not as frequently maintained as in the past, `nlme` is still more flexible than `lme4` when it comes to the modeling options it allows for. Compared to `lme4` the syntax is a bit different, but not by much.

## `nlme` implementation

```{r nlme-implementation-1}
mlm.1 <- lme(fixed = Reaction ~ 1 + Days,
             random = ~ 1 + Days | Subject,
             data = sleepstudy,
             method = "REML")
summary(mlm.1)
```

Notice how the random effects are reported. What are the similarities and what are the differences compared to `lmer()`?

In this new package, on which we will rely for the rest of today's session, we can even specify the form of the variance-covariance matrix for the residuals.

```{r nlme-implementation-2}
mlm.2 <- lme(fixed = Reaction ~ 1 + Days,
             random = ~ 1 | Subject,
             data = sleepstudy,
             method = "REML",
             correlation = corAR1(form = ~ 1 | Subject),
             control = lmeControl(maxIter = 10000,
                                  msMaxIter = 10000))
summary(mlm.2)
```

```{r model-fit-1}
anova(mlm.1, mlm.2)
rm(mlm.0, mlm.1, mlm.2, sleepstudy)
```

The model with the correlation structure specified clearly fits the data better.

# Employment outcomes study

The data comes from the National Longitudinal Study of Youth in the US, and tracks the employment outcomes of male high-school dropouts in the 1990s. It has the kind of structure that is frequently encountered in practice: irregular spacing between measurement rounds, and measurements performed at irregular dates within a wave.

## Read data

```{r read-data-2}
link <- "https://stats.idre.ucla.edu/stat/r/examples/alda/data/wages_pp.txt"
df_wages <- read.table(file = link,
                       header = TRUE,
                       sep = ",")

head(df_wages)
```

Data structure:

1. `id`: person ID
2. `lnw`: natural log of wages, in constant 1990 USD ($log_e(wages)$)
3. `exper`: years in labor force to nearest day
4. `ged`: indicator (1 = attained GED; 0 otherwise) (the GED is a test that checks whether a person has skills at the level of a high-school graduate in the US or Canada - it can replace a high-school degree from these countries)
5. `postexp`: years in labor force from day of GED attainment
6. `black`: racial background (1 = black; 0 = otherwise)
7. `hispanic`: hispanic indicator (1 = hispanic; 0 = otherwise)
8. `hgc`: highest grade completed
9. `hgc.9`: highest grade completed, centered on grade 9
10. `uerate`: unemployment rate in the local geographical area
11. `ue.7`: unemployment rate, centered on 7%
12. `ue.centert1`: unemployment rate, centered around the unemployment value at $t_1$
13. `ue.mean`: within-person mean of unemployment rate
14. `ue.person.cen`: unemployment rate, within-person centering
15. `ue.1`: unemployment rate at $t_1$

```{r group-sizes, results='asis'}
df_agg <- df_wages %>%
     group_by(id) %>%
     summarise(N = n())

table(df_agg$N) %>%
    as.data.frame() %>%
    kable(caption = "Number of observations by individual",
          caption.above = TRUE,
          col.names = c("Times measured", "N individuals"),
          row.names = FALSE)
rm(df_agg)
```

Individual measurement occasions vary between IDs. Some individuals are reached for more than 8 or 9 times, whereas a few are reached 3 times or fewer.

**Quick task**: Select from the data set a random sample of 48 individuals, and plot their observations in a panel grid of 6 rows and 8 columns. Put `exper` on the X axis, and `lnw` on the Y axis. For each panel, fit a linear fit line, so as to examine the trend in wages over time.

## Unconditional growth

We can start the modeling from the unconditional growth model.

```{r mlm-growth-1}
mlm.1 <- lme(fixed = lnw ~ 1 + exper,
             random = ~ 1 + exper | id,
             data = df_wages,
             method = "ML")
summary(mlm.1)
```

Compared to the previous example, though, we now have an individual-level, time-invariant predictor: a person's racial background.

```{r check-data-level-1}
df_agg <- df_wages %>%
    group_by(id) %>%
    summarise(black_sd = sd(black, na.rm = TRUE))
table(df_agg$black_sd)
rm(df_agg)
```

**Question**: What is another way in which you can check if a variable truly is measured at the L2 or not?

```{r mlm-growth-2}
mlm.2 <- lme(fixed = lnw ~ 1 + exper + black,
             random = ~ 1 + exper | id,
             data = df_wages,
             method = "ML")
summary(mlm.2)
```

```{r model-fit-2}
anova(mlm.1, mlm.2)
```

## Expanding model

```{r mlm-growth-3}
mlm.3 <- lme(fixed = lnw ~ 1 + exper + black + black * exper,
             random = ~ 1 + exper | id,
             data = df_wages,
             method = "ML")
summary(mlm.3)
```

```{r model-fit-3}
anova(mlm.2, mlm.3)
```

This last specification looks a lot like types of models that we tried in the last week in course. How would you interpret, in the context of a longitudinal model:

1. the effect of `exper`?
2. the effect of `black`?
3. the effect of the cross-level interaction?

We can improve on the specification above by adding another factor: unemployment in the local geographic area, centered around the value of 7%.

```{r mlm-growth-4}
mlm.4 <- lme(fixed = lnw ~ 1 + exper + black + black * exper +
                 ue.7,
             random = ~ 1 + exper | id,
             data = df_wages,
             method = "ML")
summary(mlm.4)
```

## Effect plots

Plotting the predicted trajectories of a typical Black and non-Black person after entering the job market

```{r effect-plot-1, fig.height=5, fig.width=7, dpi=144}
dat1 <- ggpredict(mlm.3,
                  terms = c("exper", "black [0, 1]"),
                  ci.lvl = 0.95,
                  type = "fe")

plot(dat1,
     show.title = FALSE,
     show.x.title = FALSE,
     show.y.title = FALSE) +
    scale_x_continuous(name = "Labor-market experience") +
    scale_y_continuous(name = "Logarithm of hourly wages") +
    theme_clean()
rm(dat1)
```

Include the effect of a dichotomous L1 predictor, to see whether getting a GED impacts one's wages.

```{r mlm-growth-5}
mlm.5 <- lme(fixed = lnw ~ 1 + exper + black + black * exper +
                 ue.7 + ged,
             random = ~ 1 + exper | id,
             data = df_wages,
             method = "ML")
summary(mlm.5)
```

Check whether the effect of experience on wages also changes after obtaining the GED. This is a time-varying 

```{r check-data-level-2, results='asis'}
df_wages %>%
    group_by(id) %>%
    summarise(ged = mean(ged, na.rm = TRUE)) %>%
    ungroup() %>%
    slice(1:10) %>%
    kable(caption = "GED completion by individual",
          caption.above = TRUE,
          digits = 3,
          columns = c("ID", "GED completion"))
```

```{r mlm-growth-6}
mlm.6 <- lme(fixed = lnw ~ 1 + exper + black + black * exper +
                 ue.7 + ged + postexp,
             random = ~ 1 + exper | id,
             data = df_wages,
             method = "ML")
summary(mlm.6)
```

How do you interpret the (lack of) effect on the `postexp` indicator? Keep in mind how this variable is constructed.

```{r effect-plot-2, fig.height=5, fig.width=7, dpi=144}
dat2 <- ggpredict(mlm.5,
                  terms = c("exper", "ged [0, 1]"),
                  ci.lvl = 0.95,
                  type = "fe")
plot(dat2,
     show.title = FALSE,
     show.x.title = FALSE,
     show.y.title = FALSE) +
    scale_x_continuous(name = "Labor-market experience") +
    scale_y_continuous(name = "Logarithm of hourly wages") +
    theme_clean()
rm(dat2)
```

# Package versions

Package versions used in this script.

```{r package-versions}
sessionInfo()
```